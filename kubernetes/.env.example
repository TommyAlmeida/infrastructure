# AWS primitives.
AWS_REGION="change me"
AWS_ACCESS_KEY_ID="change me"
AWS_SECRET_ACCESS_KEY="change me"

# The domain name to use for your Kubernetes cluster.
# If you're using AWS, a Route 53 record will be created in the same zone
# as DOMAIN_NAME for your control plane load balancer.
DOMAIN_NAME="change me"

# A unique environment name to use for storing deployment configurations
# (Terraform states, certificates and SSH keys in S3, etc.)
ENVIRONMENT_NAME="change me"

# This deployment uses one cluster across multiple AZs. This specifies
# the number of AZs to use for this cluster.
# NOTE: This deployment will fail in awful ways if NUMBER_OF_AVAILABILITY_ZONES
#       exceeds the number of availability zones available to you in your region.
# Interesting follow-up convo: https://github.com/kubernetes/kubernetes/issues/13056
NUMBER_OF_AVAILABILITY_ZONES="change me"

# The number of workers to deploy. This will deploy
# NUMBER_OF_AVAILABILITY_ZONES*NUMBER_OF_AVAILABILITY_ZONES number of
# instances.
NUMBER_OF_WORKERS_PER_CLUSTER="change me"

# The AWS S3 bucket to use for storing SSH keys.
SSH_KEY_S3_BUCKET_NAME="change me"
SSH_KEY_S3_KEY_PATH="change me"

# The user name to use for SSHing into instances.
SSH_USER_NAME="change me"

# Information to use in the X.509 certificates created for your cluster.
CA_KEY_S3_BUCKET_NAME="change me"
CA_KEY_S3_KEY_PATH="change me"
CA_CSR_CITY="change me"
CA_CSR_STATE="change me"
CA_CSR_COUNTRY_INITIALS="change me"
CA_CSR_ORGANIZATION="change me"
CA_CSR_COMMON_NAME="change me"

# A name to use for your cluster.
KUBERNETES_CLUSTER_NAME="change me"

# The version of Kubernetes to use.
KUBERNETES_VERSION="change me"

# The CIDR to use for Pods. Only /16's are supported for this deployment.
# Subnet CIDRs for your Pods will be calculated automatically.
# The service IP defaults to 10.32.0.10.
KUBERNETES_POD_CIDR="change me"

# The public port to use for your Kubernetes load balancer.
# Only 6443 was tested at this time of writing.
KUBERNETES_PUBLIC_PORT="change me"

# Because the configuration management used for this deployment
# was designed to be agnostic, every node clones a copy of this repository
# on first-boot and creates a systemd service to initialize this code.
# (See how in the Makefile.)
# This controls the branch that each node should use during this process.
KUBERNETES_GIT_BRANCH="change me"

# S3 buckets to use for storing certificates.
# This deployment appends a unique token to each certificate that is then
# copied to the user-data of each worker and controller. This prevents
# nodes from getting stale certificates during re-deploys.
# This is also not emptied out automatically, so you'll need to purge
# this from time to time.
KUBERNETES_CERTIFICATE_S3_BUCKET="change me"
KUBERNETES_CERTIFICATE_S3_KEY="change me"

# For those using the Ansible playbooks included in this repository,
# these keys specify the S3 bucket to store auto-generated Ansible variables.
ANSIBLE_VARS_S3_BUCKET="change me"
ANSIBLE_VARS_S3_KEY="change me"

# S3 buckets to use for storing Terraform state and other metadata.
TERRAFORM_STATE_S3_BUCKET="change me"
TERRAFORM_STATE_S3_KEY="change me"
